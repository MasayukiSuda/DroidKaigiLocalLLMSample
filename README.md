# DroidKaigi Local LLM Sample

## モデルファイルの配置方法

このアプリでは、LLMモデルファイルをアプリ内からダウンロードできます。

### モデルのダウンロード方法

1. アプリを起動
2. 画面上部のダウンロードアイコン（↓）をタップして「モデル管理」画面を開く
3. 使用したいモデルの「ダウンロード」ボタンをタップ
4. ダウンロードが完了するまで待機（進捗バーが表示されます）

### 利用可能なモデル

以下のモデルがダウンロード可能です：

1. **Llama 3.2 3B Instruct Q4_K_M**（推奨）
   - サイズ: 約2.3GB
   - 用途: 高品質と端末負荷のバランスが良い
   - ダウンロード元（参考）: [Hugging Face (bartowski)](https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf)

2. **TinyLlama 1.1B Q4**（軽量モデル）
   - サイズ: 約640MB
   - 用途: 軽量で高速な推論、テスト用

3. **Phi-2 Q4**
   - サイズ: 約1.5GB
   - 用途: Microsoft製の中規模モデル

### 注意事項

- モデルファイルは大容量のため、Wi-Fi環境でのダウンロードを推奨します
- ダウンロードしたモデルはアプリの内部ストレージに保存されます
- ストレージ容量に注意してください（Llama 3.2で約2.3GB、TinyLlamaで約640MB など）
- アプリをアンインストールするとダウンロードしたモデルも削除されます

## Session Proposal
```
生成AIはクラウドだけのものではありません。Android 16 以降では AICore 上の Gemini Nano を ML Kit GenAI API 経由で呼び出せ、数百ms で要約・校正・画像キャプションを端末内で完結できます。一方 OSS 界隈では ggml／llama.cpp による量子化 Llama 3 などを JNI 経由で組み込む手法や、汎用ランタイム LiteRT（旧 TensorFlowLite） にモデルを変換し NNAPI／GPU で推論するルートも整備されました。

本セッションでは「オフライン AI チャット」「リアルタイム文章要約」「リアルタイム文章校正」を一つの Compose アプリに統合しながら、三つのオンデバイスLLMを 同じプロンプト・同じ端末 でベンチマークします。

比較軸は下記の5点です。
①導入工数とビルド手順
②モデルサイズ／RAM 使用量
③推論レイテンシ
④バッテリー消費
⑤ライセンスと運用

Gemini Nano の省電力性と高レベル API の手軽さ、llama.cpp の自由度と落とし穴、LiteRT の柔軟性と量子化チューニングの難しさを可視化します。
また、それぞれの実装方法同時に紹介します。

オフラインでも瞬時に動き、個人情報をクラウドへ送らず、運用コストを抑えられるオンデバイスLLMは今後の発展が見込まれます。

本セッションを通じて、より実用的なオンデバイスLLMを活用した Android アプリ開発のイメージを掴むことができます。実際のユースケースや実装方法を知ることで、新たなアプリの発想や、既存アプリの進化のきっかけとなることを目指します。
```
