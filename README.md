# DroidKaigi Local LLM Sample

## モデルファイルの配置方法

このアプリでは、LLMモデルファイルをアプリ内からダウンロードできます。

### モデルのダウンロード方法

1. アプリを起動
2. 画面上部のダウンロードアイコン（↓）をタップして「モデル管理」画面を開く
3. 使用したいモデルの「ダウンロード」ボタンをタップ
4. ダウンロードが完了するまで待機（進捗バーが表示されます）

### 利用可能なモデル

以下のモデルがダウンロード可能です：

1. **TinyLlama 1.1B Q4** (推奨 - 軽量モデル)
   - サイズ: 約640MB
   - 用途: 軽量で高速な推論、テスト用

2. **Llama 2 7B Chat Q4** (大容量モデル)
   - サイズ: 約3.8GB  
   - 用途: より高品質な生成、本格的な利用

3. **Phi-2 Q4**
   - サイズ: 約1.5GB
   - 用途: Microsoft製の中規模モデル

### 注意事項

- モデルファイルは大容量のため、Wi-Fi環境でのダウンロードを推奨します
- ダウンロードしたモデルはアプリの内部ストレージに保存されます
- ストレージ容量に注意してください（TinyLlamaで約640MB、Llama 2で約3.8GB必要）
- アプリをアンインストールするとダウンロードしたモデルも削除されます

## Session Proposal
```
生成AIはクラウドだけのものではありません。Android 16 以降では AICore 上の Gemini Nano を ML Kit GenAI API 経由で呼び出せ、数百ms で要約・校正・画像キャプションを端末内で完結できます。一方 OSS 界隈では ggml／llama.cpp による量子化 Llama 3 などを JNI 経由で組み込む手法や、汎用ランタイム LiteRT（旧 TensorFlowLite） にモデルを変換し NNAPI／GPU で推論するルートも整備されました。

本セッションでは「オフライン AI チャット」「リアルタイム文章要約」「リアルタイム文章校正」を一つの Compose アプリに統合しながら、三つのオンデバイスLLMを 同じプロンプト・同じ端末 でベンチマークします。

比較軸は下記の5点です。
①導入工数とビルド手順
②モデルサイズ／RAM 使用量
③推論レイテンシ
④バッテリー消費
⑤ライセンスと運用

Gemini Nano の省電力性と高レベル API の手軽さ、llama.cpp の自由度と落とし穴、LiteRT の柔軟性と量子化チューニングの難しさを可視化します。
また、それぞれの実装方法同時に紹介します。

オフラインでも瞬時に動き、個人情報をクラウドへ送らず、運用コストを抑えられるオンデバイスLLMは今後の発展が見込まれます。

本セッションを通じて、より実用的なオンデバイスLLMを活用した Android アプリ開発のイメージを掴むことができます。実際のユースケースや実装方法を知ることで、新たなアプリの発想や、既存アプリの進化のきっかけとなることを目指します。
```
