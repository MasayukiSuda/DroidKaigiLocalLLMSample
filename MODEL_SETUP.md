# モデルファイルのセットアップガイド

このアプリは3つのオンデバイスLLMプロバイダーをサポートしており、実際のモデルファイルをダウンロードして使用できます。

## 対応モデル

### Llama.cpp (GGUF形式)
1. **Llama 3.2 3B Instruct Q4_K_M**（推奨）
   - サイズ: 約2.3GB
   - ダウンロード: アプリ内の設定画面から

2. **Llama 3.2 1B Instruct Q4_K_M**（軽量）
   - サイズ: 約890MB
   - ダウンロード: アプリ内の設定画面から

3. **Llama 3.1 8B Instruct Q4_K_M**（高性能）
   - サイズ: 約4.5GB
   - ダウンロード: アプリ内の設定画面から

4. **Qwen2.5 3B Instruct Q4_K_M**（多言語）
   - サイズ: 約2GB
   - ダウンロード: アプリ内の設定画面から

5. **TinyLlama 1.1B Q4**（軽量テスト用）
   - サイズ: 約640MB
   - ダウンロード: アプリ内の設定画面から

6. **Phi-2 Q4**
   - サイズ: 約1.5GB
   - ダウンロード: アプリ内の設定画面から

7. **Phi-3.5 Mini Instruct Q4_K_M**（改良版）
   - サイズ: 約2.2GB
   - ダウンロード: アプリ内の設定画面から

### LiteRT (TensorFlow Lite)
1. **Gemma3 2B (.task)**（推奨・軽量）
   - サイズ: 約1.5GB
   - ダウンロード: なし（手動配置）
   - 特徴: 効率的で軽量、モバイル最適化

2. **Gemma3 9B (.task)**（高性能）
   - サイズ: 約5GB
   - ダウンロード: なし（手動配置）
   - 特徴: 高い推論能力、複雑なタスクに適している

3. **Gemma3 27B (.task)**（最高性能）
   - サイズ: 約15GB
   - ダウンロード: なし（手動配置）
   - 特徴: 最高レベルの推論能力、大容量端末向け

**配置手順:**
- `app/src/main/assets/models/lite_rt/gemma3/` に Gemma3 の `.task` または `.tflite` ファイルを配置
- ファイル名に `2b`, `9b`, `27b` を含めるとサイズ自動識別されます
- 可能なら `encoder.json` を同ディレクトリに配置（暫定トークナイザ用）
- アプリ初回起動時に `files/models/lite_rt/gemma3/` へコピーされ、そこからロードされます

### ML Kit GenAI API (Gemini Nano)
- Android 16+ デバイスで自動的に利用可能
- 追加のダウンロード不要

## セットアップ手順

### 1. アプリ内でのモデルダウンロード

1. アプリを起動
2. 右上の設定ボタンをタップ
3. 使用したいLLMプロバイダーを選択
4. 利用可能なモデル一覧から、ダウンロードボタンをタップ
5. ダウンロード完了を待つ

### 2. 手動でのモデル配置（上級者向け）

モデルファイルを手動で配置する場合：

```
/data/data/com.daasuu.llmsample/files/models/
├── llama_cpp/
│   ├── llama-3.2-3b-instruct-q4_k_m.gguf
│   ├── tinyllama-1.1b-q4.gguf
│   └── phi-2-q4.gguf
├── lite_rt/
│   └── gemma3/
│       ├── <your-gemma3-model>.tflite
│       └── encoder.json (任意)
```

### 3. 実際のllama.cppビルド（開発者向け）

実際のllama.cppライブラリをビルドする場合：

```bash
# llama.cppのセットアップ
./setup_llama_cpp.sh

# NDKビルドの有効化（app/build.gradle.kts）
externalNativeBuild {
    cmake {
        path = file("src/main/cpp/CMakeLists.txt")
        version = "3.22.1"
    }
}

# ビルド実行
./gradlew assembleDebug
```

## モデル比較機能

### 比較画面
アプリの「比較」タブから、複数のモデルを同時実行して性能と品質を比較できます：

1. **プロンプト入力**: 同じプロンプトで複数モデルを実行
2. **リアルタイム実行**: 各モデルの生成過程をリアルタイムで比較
3. **性能指標**: レスポンス時間、トークン/秒、メモリ使用量を測定
4. **品質評価**: 生成された回答の内容と品質を比較

### ベンチマーク機能
「ベンチマーク」ボタンから詳細な性能測定が可能：

- **モデル比較ベンチマーク**: 異なるサイズ・種類のモデル性能比較
- **軽量モデル比較**: 1B-3Bクラスの軽量モデル効率性比較  
- **大型モデル比較**: 8B以上の大型モデル推論能力比較
- **包括的ベンチマーク**: 全機能・全プロバイダーの詳細比較

## パフォーマンス比較

各プロバイダーの特徴：

| プロバイダー | レイテンシ | メモリ使用量 | バッテリー | 利用可否 |
|------------|-----------|------------|-----------|----------|
| Gemini Nano | 高速 | 低 | 省電力 | Android 16+ |
| LiteRT/Gemma3 | 中速 | 中 | 中 | 全Android |
| llama.cpp | 低速 | 高 | 高消費 | 全Android |

### モデルサイズ別推奨用途

| サイズ | 推奨モデル | 用途 | 特徴 |
|--------|-----------|------|------|
| 1B-2B | TinyLlama, Gemma3-2B | 簡単な質問、基本対話 | 高速、省電力 |
| 3B | Llama3.2-3B, Qwen2.5-3B | 一般的なタスク、要約 | バランス良好 |
| 8B-9B | Llama3.1-8B, Gemma3-9B | 複雑な推論、技術解説 | 高品質出力 |
| 27B+ | Gemma3-27B | 高度な推論、創作 | 最高品質 |

## トラブルシューティング

### ダウンロードエラー
- インターネット接続を確認
- ストレージ容量を確認
- アプリを再起動

### モデル読み込みエラー
- ファイルの整合性を確認
- アプリのキャッシュをクリア
- モデルを再ダウンロード

### パフォーマンス問題
- デバイスのRAM容量を確認
- バックグラウンドアプリを終了
- より軽量なモデルを選択

## 推奨セットアップ

初回利用時の推奨手順：

1. **TinyLlama 1.1B Q4**をダウンロード（軽量で動作確認に最適）
2. チャット機能で基本動作を確認
3. より大きなモデルを試す
4. ベンチマーク機能で性能比較

これにより、DroidKaigi Local LLM Sampleの全機能を体験できます。